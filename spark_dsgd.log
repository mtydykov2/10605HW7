Traceback (most recent call last):
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 359, in <module>
    float(sys.argv[4]), float(sys.argv[5]), sys.argv[6], sys.argv[7], sys.argv[8], None)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 355, in run_all
    run_from_file(textfiles, num_iterations, beta_value, lambda_value, num_factors, num_workers, output_H_filepath, output_W_filepath, sc, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 318, in run_from_file
    factor_h, factor_w, total_squared_error_for_iteration = setup_and_run(tuples, num_movies, num_users, num_iterations, beta_value, lambda_value, num_factors, num_workers, output_H_filepath, output_W_filepath, sc, False, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 337, in setup_and_run
    factor_h, factor_w, total_squared_error_for_iteration = run_sgd(w_rdd, h_rdd, num_iterations, beta_value, lambda_value, num_workers, tuples, num_ratings_per_user, num_ratings_per_movie, num_users, num_movies, get_error, sc, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 176, in run_sgd
    new_h_map = new_rdd.filter(lambda x: len(x) > 2 and x[2] == "movies" and x[3] == worker_num).map(lambda x: (x[0], x[1]), True).collectAsMap()
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/rdd.py", line 1446, in collectAsMap
    return dict(self.collect())
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/rdd.py", line 701, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File "/usr/local/lib/python2.7/dist-packages/py4j-0.8.2.1-py2.7.egg/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/usr/local/lib/python2.7/dist-packages/py4j-0.8.2.1-py2.7.egg/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o216.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 43, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/worker.py", line 101, in main
    process()
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 98, in update_matrices
    needed_map = broadcasted_maps[(iteration_ranges_b.value[stratum_num_b.value][worker_num]["min_movie"],iteration_ranges_b.value[stratum_num_b.value][worker_num]["max_movie"])].value
AttributeError: 'NoneType' object has no attribute 'value'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

Traceback (most recent call last):
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 359, in <module>
    float(sys.argv[4]), float(sys.argv[5]), sys.argv[6], sys.argv[7], sys.argv[8], None)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 355, in run_all
    run_from_file(textfiles, num_iterations, beta_value, lambda_value, num_factors, num_workers, output_H_filepath, output_W_filepath, sc, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 318, in run_from_file
    factor_h, factor_w, total_squared_error_for_iteration = setup_and_run(tuples, num_movies, num_users, num_iterations, beta_value, lambda_value, num_factors, num_workers, output_H_filepath, output_W_filepath, sc, False, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 337, in setup_and_run
    factor_h, factor_w, total_squared_error_for_iteration = run_sgd(w_rdd, h_rdd, num_iterations, beta_value, lambda_value, num_workers, tuples, num_ratings_per_user, num_ratings_per_movie, num_users, num_movies, get_error, sc, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 176, in run_sgd
    new_h_map = new_rdd.filter(lambda x: len(x) > 2 and x[2] == "movies" and x[3] == worker_num).map(lambda x: (x[0], x[1]), True).collectAsMap()
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/rdd.py", line 1446, in collectAsMap
    return dict(self.collect())
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/rdd.py", line 701, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File "/usr/local/lib/python2.7/dist-packages/py4j-0.8.2.1-py2.7.egg/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/usr/local/lib/python2.7/dist-packages/py4j-0.8.2.1-py2.7.egg/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o216.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 43, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/worker.py", line 101, in main
    process()
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 98, in update_matrices
    needed_map = broadcasted_maps[(iteration_ranges_b.value[stratum_num_b.value][worker_num]["min_movie"],iteration_ranges_b.value[stratum_num_b.value][worker_num]["max_movie"])].value
AttributeError: 'NoneType' object has no attribute 'value'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

Traceback (most recent call last):
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 359, in <module>
    float(sys.argv[4]), float(sys.argv[5]), sys.argv[6], sys.argv[7], sys.argv[8], None)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 355, in run_all
    run_from_file(textfiles, num_iterations, beta_value, lambda_value, num_factors, num_workers, output_H_filepath, output_W_filepath, sc, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 318, in run_from_file
    factor_h, factor_w, total_squared_error_for_iteration = setup_and_run(tuples, num_movies, num_users, num_iterations, beta_value, lambda_value, num_factors, num_workers, output_H_filepath, output_W_filepath, sc, False, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 337, in setup_and_run
    factor_h, factor_w, total_squared_error_for_iteration = run_sgd(w_rdd, h_rdd, num_iterations, beta_value, lambda_value, num_workers, tuples, num_ratings_per_user, num_ratings_per_movie, num_users, num_movies, get_error, sc, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 176, in run_sgd
    new_h_map = new_rdd.filter(lambda x: len(x) > 2 and x[2] == "movies" and x[3] == worker_num).map(lambda x: (x[0], x[1]), True).collectAsMap()
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/rdd.py", line 1446, in collectAsMap
    return dict(self.collect())
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/rdd.py", line 701, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File "/usr/local/lib/python2.7/dist-packages/py4j-0.8.2.1-py2.7.egg/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/usr/local/lib/python2.7/dist-packages/py4j-0.8.2.1-py2.7.egg/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o216.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 43, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/worker.py", line 101, in main
    process()
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 98, in update_matrices
    needed_map = broadcasted_maps[(iteration_ranges_b.value[stratum_num_b.value][worker_num]["min_movie"],iteration_ranges_b.value[stratum_num_b.value][worker_num]["max_movie"])].value
AttributeError: 'NoneType' object has no attribute 'value'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

Traceback (most recent call last):
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 359, in <module>
    float(sys.argv[4]), float(sys.argv[5]), sys.argv[6], sys.argv[7], sys.argv[8], None)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 355, in run_all
    run_from_file(textfiles, num_iterations, beta_value, lambda_value, num_factors, num_workers, output_H_filepath, output_W_filepath, sc, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 318, in run_from_file
    factor_h, factor_w, total_squared_error_for_iteration = setup_and_run(tuples, num_movies, num_users, num_iterations, beta_value, lambda_value, num_factors, num_workers, output_H_filepath, output_W_filepath, sc, False, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 337, in setup_and_run
    factor_h, factor_w, total_squared_error_for_iteration = run_sgd(w_rdd, h_rdd, num_iterations, beta_value, lambda_value, num_workers, tuples, num_ratings_per_user, num_ratings_per_movie, num_users, num_movies, get_error, sc, errorfile)
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 176, in run_sgd
    new_h_map = new_rdd.filter(lambda x: len(x) > 2 and x[2] == "movies" and x[3] == worker_num).map(lambda x: (x[0], x[1]), True).collectAsMap()
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/rdd.py", line 1446, in collectAsMap
    return dict(self.collect())
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/rdd.py", line 701, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File "/usr/local/lib/python2.7/dist-packages/py4j-0.8.2.1-py2.7.egg/py4j/java_gateway.py", line 538, in __call__
    self.target_id, self.name)
  File "/usr/local/lib/python2.7/dist-packages/py4j-0.8.2.1-py2.7.egg/py4j/protocol.py", line 300, in get_return_value
    format(target_id, '.', name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o216.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 43, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/worker.py", line 101, in main
    process()
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/worker.py", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/mtydykov/Downloads/spark/spark-1.3.0/python/pyspark/serializers.py", line 236, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/mtydykov/workspace/10605HW7/dsgd_mf.py", line 98, in update_matrices
    needed_map = broadcasted_maps[(iteration_ranges_b.value[stratum_num_b.value][worker_num]["min_movie"],iteration_ranges_b.value[stratum_num_b.value][worker_num]["max_movie"])].value
AttributeError: 'NoneType' object has no attribute 'value'

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

---------------------------------------------------
Autograding ...
---------------------------------------------------
The stderr will be logged to file  eval_acc.log
command you are executing  /home/mtydykov/Downloads/spark/spark-1.3.0/bin/spark-submit dsgd_mf.py 20 3 100 0.9 0.1 autolab_train.csv w.csv h.csv
Analyzing your memory usage and Efficiency..

testing  1

time (s): 6291.11408877
memory (byte): 764664
testing  2

time (s): 26.3066759109
memory (byte): 764664
testing  3

time (s): 28.6276009083
memory (byte): 764664
testing  4

time (s): 24.6826360226
memory (byte): 764664
testing  5

time (s): 25.5490131378
memory (byte): 764664


average time usage (s): 1279.25600295
average memory usage (byte): 764664
